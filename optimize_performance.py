"""
Performance Optimization Script for Equity Valuation Platform
Applies caching, optimization, and performance enhancements
"""

import streamlit as st
import pandas as pd
import numpy as np
import time
from functools import wraps
import hashlib
import pickle
import os
from typing import Any, Callable, Dict
import logging

class PerformanceOptimizer:
    """Handles performance optimization for the platform"""
    
    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
    def smart_cache(self, ttl: int = 3600):
        """Smart caching decorator with TTL"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                # Create cache key from function name and arguments
                cache_key = self._create_cache_key(func.__name__, args, kwargs)
                cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
                
                # Check if cache exists and is valid
                if os.path.exists(cache_file):
                    cache_time = os.path.getmtime(cache_file)
                    if time.time() - cache_time < ttl:
                        try:
                            with open(cache_file, 'rb') as f:
                                return pickle.load(f)
                        except:
                            pass
                
                # Cache miss or expired - compute result
                result = func(*args, **kwargs)
                
                # Save to cache
                try:
                    with open(cache_file, 'wb') as f:
                        pickle.dump(result, f)
                except:
                    pass  # Fail silently if caching fails
                
                return result
            return wrapper
        return decorator
    
    def _create_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """Create a unique cache key"""
        key_data = f"{func_name}_{str(args)}_{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def optimize_dataframes(self, df: pd.DataFrame) -> pd.DataFrame:
        """Optimize DataFrame memory usage"""
        if df.empty:
            return df
            
        # Optimize numeric columns
        for col in df.select_dtypes(include=['int64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')
        
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
        
        # Optimize object columns
        for col in df.select_dtypes(include=['object']).columns:
            unique_count = df[col].nunique()
            total_count = len(df[col])
            if unique_count / total_count < 0.5:  # Less than 50% unique values
                df[col] = df[col].astype('category')
        
        return df
    
    def batch_api_calls(self, symbols: list, batch_size: int = 10, delay: float = 0.1):
        """Batch API calls to avoid rate limits"""
        batches = [symbols[i:i + batch_size] for i in range(0, len(symbols), batch_size)]
        
        for batch in batches:
            yield batch
            if delay > 0:
                time.sleep(delay)


# Performance-optimized versions of key functions
optimizer = PerformanceOptimizer()

@optimizer.smart_cache(ttl=1800)  # 30 minute cache
def cached_market_data(symbol: str, period: str = "1y"):
    """Cached version of market data retrieval"""
    from src.data.market_data import MarketDataProvider
    provider = MarketDataProvider()
    return provider.get_stock_data(symbol, period)

@optimizer.smart_cache(ttl=3600)  # 1 hour cache
def cached_company_info(symbol: str):
    """Cached version of company info retrieval"""
    from src.data.market_data import MarketDataProvider
    provider = MarketDataProvider()
    return provider.get_company_info(symbol)

@st.cache_data(ttl=1800, show_spinner=False)
def optimized_dcf_calculation(base_revenue: float, 
                            growth_rates: list,
                            operating_margin: float,
                            wacc: float,
                            terminal_growth: float,
                            shares_outstanding: int) -> dict:
    """Optimized DCF calculation with caching"""
    from src.models.dcf import DCFModel, DCFAssumptions
    
    assumptions = DCFAssumptions(
        base_revenue=base_revenue,
        revenue_growth_rates=growth_rates,
        operating_margin=operating_margin,
        terminal_growth_rate=terminal_growth,
        shares_outstanding=shares_outstanding
    )
    
    model = DCFModel("CACHED")
    model.set_assumptions(assumptions)
    return model.calculate_dcf_valuation()

@st.cache_data(ttl=3600, show_spinner=False)
def optimized_peer_analysis(target_symbol: str, peer_symbols: list) -> dict:
    """Optimized peer analysis with caching"""
    from src.models.comps import ComparableAnalyzer
    
    analyzer = ComparableAnalyzer(target_symbol)
    
    # Load target
    target_profile = analyzer.load_target_company()
    
    # Load peers in batches
    peer_profiles = {}
    for batch in optimizer.batch_api_calls(peer_symbols, batch_size=5):
        batch_profiles = analyzer.load_peer_data(batch)
        peer_profiles.update(batch_profiles)
    
    # Calculate analysis
    analyzer.peer_profiles = peer_profiles
    multiples_df = analyzer.calculate_multiples()
    implied_valuations = analyzer.calculate_implied_valuation()
    
    return {
        'multiples_df': multiples_df,
        'implied_valuations': implied_valuations,
        'target_profile': target_profile,
        'peer_profiles': peer_profiles
    }

def apply_streamlit_optimizations():
    """Apply Streamlit-specific optimizations"""
    
    # Configure Streamlit for better performance
    if not hasattr(st.session_state, 'performance_optimized'):
        # Set page config for optimal loading
        st.set_page_config(
            page_title="Equity Valuation Pro",
            page_icon="ðŸ“Š",
            layout="wide",
            initial_sidebar_state="expanded",
            menu_items={
                'Get Help': 'https://github.com/yourusername/equity-valuation-pro',
                'Report a bug': 'https://github.com/yourusername/equity-valuation-pro/issues',
                'About': "Professional Equity Valuation Platform"
            }
        )
        
        # Initialize performance tracking
        st.session_state.performance_optimized = True
        st.session_state.page_load_time = time.time()

def performance_monitor():
    """Add performance monitoring to the app"""
    if 'page_load_time' in st.session_state:
        load_time = time.time() - st.session_state.page_load_time
        
        # Only show performance info in sidebar for development
        with st.sidebar:
            with st.expander("âš¡ Performance", expanded=False):
                st.metric("Page Load Time", f"{load_time:.2f}s")
                
                # Memory usage info
                import psutil
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                st.metric("Memory Usage", f"{memory_mb:.1f} MB")
                
                # Cache info
                cache_dir = ".cache"
                if os.path.exists(cache_dir):
                    cache_files = len([f for f in os.listdir(cache_dir) if f.endswith('.pkl')])
                    st.metric("Cache Entries", cache_files)

# Optimized visualization functions
@st.cache_data(show_spinner=False)
def create_optimized_chart(data: pd.DataFrame, chart_type: str, **kwargs):
    """Create optimized charts with caching"""
    import plotly.graph_objects as go
    import plotly.express as px
    
    # Optimize data before charting
    if len(data) > 1000:
        # Sample data for large datasets
        data = data.sample(n=1000).sort_index()
    
    data = optimizer.optimize_dataframes(data)
    
    if chart_type == "line":
        fig = px.line(data, **kwargs)
    elif chart_type == "bar":
        fig = px.bar(data, **kwargs)
    elif chart_type == "scatter":
        fig = px.scatter(data, **kwargs)
    else:
        fig = go.Figure()
    
    # Apply consistent styling
    fig.update_layout(
        template='plotly_white',
        font=dict(family="Arial", size=12),
        margin=dict(l=20, r=20, t=40, b=20)
    )
    
    return fig

# Database connection optimization (for future use)
class DatabaseOptimizer:
    """Optimize database operations when implemented"""
    
    @staticmethod
    def create_indexes():
        """Create database indexes for common queries"""
        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_symbol ON stock_data(symbol);",
            "CREATE INDEX IF NOT EXISTS idx_date ON stock_data(date);",
            "CREATE INDEX IF NOT EXISTS idx_symbol_date ON stock_data(symbol, date);"
        ]
        return indexes
    
    @staticmethod
    def optimize_queries():
        """Optimize common database queries"""
        optimized_queries = {
            'get_latest_price': """
                SELECT close FROM stock_data 
                WHERE symbol = ? 
                ORDER BY date DESC 
                LIMIT 1
            """,
            'get_price_range': """
                SELECT date, close FROM stock_data 
                WHERE symbol = ? AND date BETWEEN ? AND ?
                ORDER BY date ASC
            """
        }
        return optimized_queries

# Error handling optimization
def optimized_error_handler(func):
    """Optimized error handling decorator"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            # Log error without exposing sensitive info
            error_id = hashlib.md5(str(e).encode()).hexdigest()[:8]
            logging.error(f"Error {error_id}: {str(e)}")
            
            # Show user-friendly error
            st.error(f"âš ï¸ An error occurred (ID: {error_id}). Please try again or contact support.")
            return None
    return wrapper

# Memory management
def cleanup_session_state():
    """Clean up old session state data"""
    current_time = time.time()
    keys_to_remove = []
    
    for key in st.session_state.keys():
        if key.startswith('temp_') and hasattr(st.session_state[key], 'timestamp'):
            if current_time - st.session_state[key].timestamp > 3600:  # 1 hour
                keys_to_remove.append(key)
    
    for key in keys_to_remove:
        del st.session_state[key]

# Precompute expensive calculations
@st.cache_data(ttl=86400)  # 24 hour cache
def precompute_market_statistics():
    """Precompute market-wide statistics"""
    # This would compute market indices, sector averages, etc.
    return {
        'sp500_pe': 20.5,
        'market_volatility': 0.18,
        'risk_free_rate': 0.035,
        'market_risk_premium': 0.065
    }

def main():
    """Apply all performance optimizations"""
    print("âš¡ Applying performance optimizations...")
    
    # Apply optimizations
    apply_streamlit_optimizations()
    
    # Clean up old cache files
    cache_dir = ".cache"
    if os.path.exists(cache_dir):
        current_time = time.time()
        for filename in os.listdir(cache_dir):
            file_path = os.path.join(cache_dir, filename)
            if current_time - os.path.getmtime(file_path) > 86400:  # 24 hours
                os.remove(file_path)
    
    print("âœ… Performance optimizations applied!")
    print("""
ðŸš€ OPTIMIZATIONS APPLIED:
- Smart caching with TTL
- DataFrame memory optimization  
- API call batching
- Streamlit configuration
- Chart optimization
- Error handling improvement
- Memory cleanup routines

ðŸ’¡ Your app will now load faster and use less memory!
    """)

if __name__ == "__main__":
    main()
